@INPROCEEDINGS{PoveyEtAl2011ASRU-KALDI,
         author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
       keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM},
          month = dec,
          title = {The Kaldi Speech Recognition Toolkit},
      booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding},
           year = {2011},
      publisher = {IEEE Signal Processing Society},
       location = {Hilton Waikoloa Village, Big Island, Hawaii, US},
           note = {IEEE Catalog No.: CFP11SRW-USB},
}

@Article {HintonEtAl2012IEEESicProc,
abstract     = {Most current speech recognition systems use hidden Markov models (HMMs) to
                deal with the temporal variability of speech and Gaussian mixture models (GMMs)
                to determine how well each state of each HMM fits a frame or a short window of
                frames of coefficients that represents the acoustic input. An alternative way to
                evaluate the fit is to use a feed-forward neural network that takes several
                frames of coefficients as input and produces posterior probabilities over HMM
                states as output. Deep neural networks (DNNs) that have many hidden layers and
                are trained using new methods have been shown to outperform GMMs on a variety of
                speech recognition benchmarks, sometimes by a large margin. This article provides
                an overview of this progress and represents the shared views of four research
                groups that have had recent successes in using DNNs for acoustic modeling in
                speech recognition.},
author       = {Geoffrey Hinton and Li Deng and Dong Yu and Abdel-rahman Mohamed and Navdeep
                Jaitly and Andrew Senior and Vincent Vanhoucke and Patrick Nguyen and Tara
                Sainath and Brian Kingsbury},
journal      = {IEEE Signal Processing Magazine},
month        = {November},
number       = {6},
pages        = {82-97},
title        = {Deep Neural Networks for Acoustic Modeling in Speech Recognition},
url          = {http://research.microsoft.com/apps/pubs/default.aspx?id=171498},
volume       = {29},
year         = {2012},
}


@Article{Schmidhuber2015NeuralNetworks,
  author = 	 {Jürgen Schmidhuber},
  title = 	 {Deep Learning in Neural Networks: An Overview},
  journal = 	 {Neural Networks},
  year = 	 {2015},
  OPTkey = 	 {},
  volume = 	 {61},
  OPTnumber = 	 {},
  pages = 	 {85--117},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.

As a machine learning researcher, I am obsessed with credit assignment. In case you know of references to add or correct, please send them with brief explanations to juergen@idsia.ch, preferably together with URL links to PDFs for verification. Between 16 April and 8 October 2014, drafts of this paper have already undergone massive open online peer review through public mailing lists including connectionists@cs.cmu.edu, ml-news@googlegroups.com, compneuro@neuroinf.org, genetic_programming@yahoogroups.com, rl-list@googlegroups.com, imageworld-@diku.dk, and Google+. Thanks to numerous experts for valuable comments!

The contents of this paper may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites.}
}

@INPROCEEDINGS{YaoEtAl2012SLTadaptation,
    author = {Kaisheng Yao and Dong Yu and Frank Seide and Hang Su and Li Deng and Yifan Gong},
    title = {Adaptation of context-dependent deep neural networks for automatic speech recognition},
    booktitle = {in Proc. SLT’12},
    year = {2012}
}

@Article{LeCunEtAl2015Nature,
  author = 	 {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title = 	 {Deep Learning},
  journal = 	 {Nature},
  year = 	 {2015},
  OPTkey = 	 {},
  volume = 	 {521},
  number = 	 {7553},
  pages = 	 {436--444},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  url = {http://dx.doi.org/10.1038/nature14539},
  doi = {10.1038/nature14539}, 
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech}
}

@ARTICLE{lee89, 
  author={Lee, K.-F. and Hon, H.-W.}, 
  journal={Acoustics, Speech and Signal Processing, IEEE Transactions on}, 
  title={Speaker-independent phone recognition using hidden Markov models}, 
  year={1989}, 
  volume={37}, 
  number={11}, 
  pages={1641--1648}, 
  keywords={Markov processes;speech recognition;HMM;LPC parameters;TIMIT database;co-occurrence smoothing algorithm;expert spectrogram readers;hidden Markov models;linear-predictive-coding;multiple codebooks;speaker-independent phone recognition;speech recognition;Acoustics;Context modeling;Databases;Hidden Markov models;Humans;Knowledge engineering;Linear predictive coding;Maximum likelihood decoding;Natural languages;Speech recognition}, 
  doi={10.1109/29.46546}, 
  ISSN={0096-3518}, 
  month={Nov},
}


@ARTICLE{hinton06,
    author = {Geoffrey E. Hinton and Simon Osindero},
    title = {A fast learning algorithm for deep belief nets},
    journal = {Neural Computation},
    year = {2006},
    volume = {18},
    pages = {2006}
} 

@INPROCEEDINGS{dbn09, 
	author={Mohamed, A. and Sainath, T.N. and Dahl, G. and Ramabhadran, B. and Hinton, G.E. and Picheny, M.A.}, 
	booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on}, 
	title={Deep Belief Networks using discriminative features for phone recognition}, 
	year={2011}, 
	pages={5060-5063}, 
	keywords={hidden Markov models;higher order statistics;speech recognition;DBN;MFCCs;TIMIT corpus;bigram language model;deep belief networks;discriminative features;higher-order statistical structure;multilayer generative model;phone recognition;trigram language model;Artificial neural networks;Decoding;Error analysis;Hidden Markov models;Mel frequency cepstral coefficient;Speech;Training;Deep belief networks;Discriminative feature transformation;Phone recognition}, 
	doi={10.1109/ICASSP.2011.5947494}, 
	ISSN={1520-6149}, 
	month={May},}


@MISC{theano1,
        author = {Bastien, Frédéric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
      abstract = {Theano is a linear algebra compiler that optimizes a user’s symbolically-speciﬁed
mathematical computations to produce efﬁcient low-level implementations. In
this paper, we present new features and efﬁciency improvements to Theano, and
benchmarks demonstrating Theano’s performance relative to Torch7, a recently
introduced machine learning library, and to RNNLM, a C++ library targeted at
recurrent neural networks.}
}

@Article{gs:Salvi2006,
  author = 	 {Giampiero Salvi},
  title = 	 {Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints},
  journal = 	 {Speech Communication},
  year = 	 {2006},
  OPTkey = 	 {},
  volume = 	 {48},
  number = 	 {7},
  pages = 	 {802--818},
  month = 	 jul,
  OPTnote = 	 {},
  OPTannote = 	 {},
  keywords =     {synface, asr, neural networks, mine},
  bibliometric = {8},
  google-scholar-id = {5804744744218397221}
}

@Article{gs:SalviEtAl2009,
  author = 	 {Giampiero Salvi and Jonas Beskow and Samer {Al Moubayed} and Björn Granström},
  title = 	 {{S}yn{F}ace --- Speech-driven Facial Animation for Virtual Speech-reading Support},
  journal = 	 {EURASIP Journal on Audio, Speech, and Music Processing},
  year = 	 {2009},
  OPTkey = 	 {},
  OPTvolume = 	 {2009},
  OPTnumber = 	 {2009},
  OPTpages = 	 {},
  month = 	 sep,
  OPTnote = 	 {},
  OPTannote = 	 {},
  bibliometric = {17},
  google-scholar-id = {}
}

@INPROCEEDINGS{MuEtAl2010LipSync, 
  author={Kaihui Mu and Jianhua Tao and Jianfeng Che and Minghao Yang}, 
  booktitle={Universal Communication Symposium (IUCS), 2010 4th International}, 
  title={Real-time speech-driven lip synchronization}, 
  year={2010}, 
  pages={378--382}, 
  keywords={cepstral analysis;computer animation;human computer interaction;rendering (computer graphics);speech processing;MPEG-4;Mel-frequency cepstral coefficients;acoustic speech signal;collaborative filtering;data-driven approach;facial animation parameter;human-computer interaction;lip movement rendering;multimodal database collection;real-time speech-driven lip synchronization processing;speech visual representation;Acoustics;Face;Hidden Markov models;Speech;Synchronization;Transform coding;Visualization;FAP;MFCC;collaborative filtering;real-time speech-driven lip synchronization}, 
  doi={10.1109/IUCS.2010.5666250}, 
  month={Oct}
}

@INPROCEEDINGS{LiEtAl2013ICASSP, 
  author={Hao Li and Minghao Yang and Jianhua Tao}, 
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on}, 
  title={Speaker-independent lips and tongue visualization of vowels}, 
  year={2013}, 
  pages={8106-8110}, 
  keywords={geometry;learning (artificial intelligence);speech synthesis;2D geometric model;DRD;EMA data;acoustic-to-articulatory inversion mapping;directional relative displacement;electromagnetic articulograph data;multispeakers acoustic-articulatory data;speaker-independent lips visualization;speaker-independent tongue visualization;speech-driven lips animation synthesis scheme;speech-driven tongue animation synthesis scheme;vowel data;Acoustics;Animation;Coils;Lips;Speech;Tongue;Vectors;acoustic-to-articulatory inversion;articulatory models;electromagnetic articulograph;speech visualization}, 
  doi={10.1109/ICASSP.2013.6639244}, 
  ISSN={1520-6149}, 
  month={May}
}

@ARTICLE{Mohamed12,
    author = {Abdel-rahman Mohamed and George E. Dahl and Geoffrey Hinton},
    title = {Acoustic modeling using deep belief networks},
    journal = {IEEE Trans. Audio, Speech, Lang. Process},
    year = {2012},
    pages = {14--22}
}

@ARTICLE{pdnn,
   author = {{Miao}, Y.},
    title = "{Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1401.6984},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computation and Language},
     year = 2014,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1401.6984M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@INPROCEEDINGS{sat14,
author={Ochiai, T. and Matsuda, S. and Xugang Lu and Hori, C. and Katagiri, S.},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
title={Speaker Adaptive Training using Deep Neural Networks},
year={2014},
pages={6349-6353},
keywords={Gaussian processes;hidden Markov models;learning (artificial intelligence);neural nets;speaker recognition;Gaussian mixture models;HMM speech recognizer;SAT;SD module;baseline SI DNN-HMM recognizer;deep neural networks;hidden-Markov-model;seven-layer DNN;speaker adaptation scheme;speaker adaptive training;speaker-dependent module;speaker-independent recognizer development;training scheme;Acoustics;Hidden Markov models;Silicon;Speech;Speech recognition;Training;Vectors;Deep Neural Network;Speaker Adaptative Training},
doi={10.1109/ICASSP.2014.6854826},
month={May},}

@INPROCEEDINGS{DahlEtAl2011ICASSP, 
author={Dahl, G.E. and Dong Yu and Li Deng and Acero, A.}, 
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on}, 
title={Large vocabulary continuous speech recognition with context-dependent DBN-HMMS}, 
year={2011}, 
pages={4688-4691}, 
keywords={Gaussian distribution;belief networks;hidden Markov models;speech recognition;Bing mobile voice search task;Gaussian mixture model;ML criteria;MPE;context-dependent DBN-HMMS;context-independent deep belief network;hidden Markov model hybrid architecture;large vocabulary continuous speech recognition;maximum likelihood criteria;minimum phone error rate;Accuracy;Acoustics;Artificial neural networks;Hidden Markov models;Speech recognition;Training;Vocabulary;DBN-HMM;LVCSR;Speech recognition;context-dependent phone;deep belief network}, 
doi={10.1109/ICASSP.2011.5947401}, 
ISSN={1520-6149}, 
month={May},}

@INPROCEEDINGS{danp12, 
author={Povey, D. and Hannemann, M. and Boulianne, G. and Burget, L. and Ghoshal, A. and Janda, M. and Karafiat, M. and Kombrink, S. and Motlicek, P. and Yanmin Qian and Riedhammer, K. and Vesely, K. and Ngoc Thang Vu}, 
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on}, 
title={Generating exact lattices in the WFST framework}, 
year={2012}, 
pages={4213-4216}, 
keywords={decoding;hidden Markov models;transducers;HMM transitions;HMM-state-level alignments;WFST decoders;WFST framework;alternative transcriptions;exact lattice generation;input-symbol-sequence;natural properties;special determinization;state-level lattice;utterance;weighted finite state transducer;word sequence;Acoustic beams;Acoustics;Decoding;Educational institutions;Hidden Markov models;Lattices;Viterbi algorithm;Lattice Generation;Speech Recognition}, 
doi={10.1109/ICASSP.2012.6288848}, 
ISSN={1520-6149}, 
month={March},}

@InProceedings{LiEtAl2002ICSLPfMLLR,
  author = 	 {Yongxin Li and Hakan Erdogan and Yuqing Gao and Etienne Marcheret},
  title = 	 {Incremental On-Line Feature Space MLLR Adaptation for Telephony Speech Recognition},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {Proceedings of ICSLP},
  year = 	 {2002},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  pages = 	 {1417--1420},
  OPTmonth = 	 {},
  address = 	 {Denver, Colorado, USA},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@PhdThesis{Jaitly2014Thesis,
  author = 	 {Navdeep Jaitly},
  title = 	 {Exploring Deep Learning Methods for Discovering Features in Speech Signals},
  school = 	 {University of Toronto},
  year = 	 {2014},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

