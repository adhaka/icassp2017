\section{METHOD}
\label{sec:method}
 We desire a good representation of input along with good prediction and discriminative abilities from our network.
 The architecture of this network is similar to a regular auto-encoder with the notable difference being the addition of an additional supervised classifier in feedback in parallel with the regular decoder.  \\
Our model is a neural network. The network contains multiple layers of encoder-decoder-classifier system. The system is trained layer by layer.

\begin{figure}
\includegraphics[width=\columnwidth]{model}
\caption{Illustration of the method: A sequence of 11 speech feature frames constitutes the input to the neural network. The context window is not necessarily symmetric with respect to the current frame ($t_0$). In the illustration a shift of -3 was applied.}
\label{fig:model}
\end{figure}

The computational cost is linear in the number of training samples, and thus it is more efficient than graph based Semi-Supervised Learning algorithms which have $O(N^{3})$ cubic complexity. For each layer, the cost is given by a forward and backward pass through encoder, decoder and classifier. 
In the supervised setting, the cost function is generally the cross-entropy logloss or misclassification error rate. While in unsupervised learnig, the objective is to find a different and more discriminative representation of the input data. The cost function in this case is the second degree norm of difference between original input and reconstructed input, where the input and output have the same dimensions. It has been found, that adding noise to the original input by a process called 'corruption' in which some dimensions of the input vector are randomly picked and set to zero, helps the network to learn even a better representation as described in \cite{Bengio-nips-2006}. But a hybrid approach where unsupervised learning is done in a way to aid supervised learning has not been fully explored yet, although some experiments have been done in sparse settings on document analysis \cite{ranzato-2008}. Our aim will be to add these seperate costs and train the network while minimising the combined cost at once in the same epoch. This is a non convex optimisation problem where we try to minimise the cost with respect to the weights of the layers.  When the input datapoint is not accompanied by a label, the classifier part of the layer is not updated, and the loss function simply reduces to $E_{R}$.  \\

Different training procedures have been used with varying amount of success. In our case we will train each layer greedily one at a time. This means, that when the parameters of one layer have been found, the data is fed through that layer, and the output becomes the input for the training of the next layer, which is trained subsequently. With this training procedure, we do not need to run a backward pass through the entire network at once, which is generally highly computationally intensive. 
The mathematical expressions for reconstruction error cost is given in Equation~\ref{eqn:cost-reconstruct}. The cost function is the same as that of a regular auto-encoder. 

\begin{eqnarray}
\label{eqn:cost-reconstruct}
E_R = ||x - \hat{x}||^2   \\ 
E_{R} = ||z_{(n-1)} - W_{D}z_{(n)} -b_{D}||_{2}^{2}.
\end{eqnarray}
The weight matrices, $W_{E}, W_{D}, W_{C}$ and the bias values, $b_{E}, b_{D}, b_{C}$ are then calculated by performing gradient descent of the objective function with respect to all these parameters mentioned. $W_{E}$ is the weight matrix for the encoder, $W_{D}$ is the weight matrix for decoder, $W_{C}$ is the classifier weight matrix. \\
The cost for misclassification contributed by the classifier part of the network~\ref{fig:model-design} is given  by Equation~\ref{eqn:cost-classify}. The combined cost is given by Equation~\ref{eqn:cost-combined}. 
\begin{equation}
\label{eqn:cost-classify}
E_C = -\sum{y_{i}\log h_{i}}
\end{equation}
\begin{equation}
\label{eqn:cost-combined}
E = E_R + \alpha E_C
\end{equation}
where $x$ denotes original input, $\hat{x}$ represents reconstructed input, $y$ represent target labels and $h$ denotes softmax output of the last layer. This model can be iteratively applied to as many layers as we want. However, in our experiments, we use only a single layer for feature representation. We use mini-batch SGD as explained in the theory section as the optimizer over this cost function. The update of encoder weight at each iteration  is given by equation \ref{eqn:sgd}. It is important to note that the update of encoder weight $W_{E}$ is dependent both on $W_{D}$ and $W_{C}$, and the delta propagated in the backpropagation algorithm will be a linear combination of the deltas calculated in both parts. We will call our model Semi-Supervised Sparse Autoencoder (SSSAE). Another interesting thing to note is that generally in practice, autoencoders are preferred to have "tied weights". This essentially means that the decoder weight matrix is the transpose of encoder weight matrix. So in each update, as the encoder weight is modified, the decoder weight matrix is also modified such that it is the transpose of the new encoder weight matrix. This reduces the amount of free parameters available. But in our experiments, the decoder weight matrix is free and not bound by the encoder weight matrix. This makes our model more expressive at the cost of more computational overhead and possible delayed convergence.    \\
Sparse Autoencoders have generally performed well in a lot of applications. But it has some disadvantages too. Sparse autoencoders 
require more memory as there are more nodes in the hidden layers, and the presence of bigger matrices slows computation down. In comparison autoencoders with bottleneck architecture have much less nodes in hidden layer and have lesser time complexity. 


%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% TeX-master: "dnnlatency"
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
