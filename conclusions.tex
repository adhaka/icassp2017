\section{CONCLUSIONS}
\label{sec:conclusions}

We can observe from our experiments that our algorithm based on learning sparse representation of data simultaneously with labeled discriminative classification, performed better than standard "supervised" training of the neural networks in terms of frame-level phoneme classification. Nevertheless, it is also important to note that, most standard and industry-grade speech processing systems need to handle units like phones, words that are inherently of variable length. In other-words, improvement in accuracy of frame-level based phoneme classification might be a good indicator, but it cannot be definitely said, that overall speech recognition accuracy when measured in terms of word-recognition will definitely increase. Though, most researchers in the community still focus on frame-level phoneme accuracy as the first goal, before moving on to the harder challenge of word recognition and variable-length "large" speech segments like sentences. 

Regarding the choice of the percentage of labeled data at which the experiments were performed, we can argue that in nature in general, we always find an abundance of unlabeled data, so it becomes more important for us to investigate our models when the percentage of labeled data is very low. Also, as the percentage of labeled datapoints increase, the accuracy starts to saturate, and any subsequent increase in percentage of labeled examples does not bring in much of improvement in the accuracy as observed in the lower percentages of data. This seems reasonable, as after a certain percentage of data, the model knows the input distribution almost completely, and only small improvements in knowledge are observed henceforth. The size of the labeled points for our experiments is varied from 1\% to 30\% of the entire training set frames count.

This thesis also discussed the pros and cons of graph-based semisupervised Learning, which is much more computationally intensive, and even then our algorithm gives competitive result in comparison to the best performing GBL.

There are many exciting ideas and research directions, in which this work could be extended. Firstly, we could use feature-bank features on feature processing from raw waveform. Though MFCCs and their deltas have been effective as features for phoneme recognition, they are highly compressed. Using feature banks instead might give a higher accuracy. One objective of using MFCCs was to compare with the results of \cite{liu-2013}, who also have used MFCCs for GBL Semi-Supervised Learning. As discussed above, a logical next step would be to apply the algorithm for segment classification task. That way, it could be definitely said that, the algorithm is good for speech processing and recognition applications. 

%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% TeX-master: "dnnlatency"
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
