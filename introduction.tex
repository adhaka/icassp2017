\section{Introduction}
\label{sec:intro}
In recent years, the development of deep neural models based on Restricted Boltzman machines (RBMs) pretraining has revitalised the use of artificial neural networks (ANNs) in automatic speech recognition (ASR) as well as in many other fields (see \cite{LeCunEtAl2015Nature, Schmidhuber2015NeuralNetworks} for extensive reviews).
A key factor that determines the usability of applications based on speech recognition is the latency or lag of the system.
In dialogue systems, e.g., long latencies may disrupt the natural turn-taking in the human-machine conversation.
In other specific applications the lag may even be more critical.
A typical example involves systems that use ASR to drive the lip movements of an avatar in real time to support telepresence \cite{gs:SalviEtAl2009, MuEtAl2010LipSync, LiEtAl2013ICASSP}.

Deep Learning has revolutionised research in Automatic Speech Recognition(ASR). Despite, the recent significant improvements made in WER(Word Error Rate), most of the work has been reported on very large fully-labeled datasets. The training procedure is either fully supervised or fully unsupervised followed by supervised. Unsupervised Learning by definition can not know beforehand, what representation will be useful for the task at hand, which is generally classification. Recently, semi-supervised learning which learns jointly from labeled data and unlabeled data has been applied to problems in Computer Vision, handwriting recognition, but there is still a dearth of research on using it in Automatic Speech Recognition (ASR). The work done by \cite{liu2013, liu2014} is one of the first attempts on using these
approaches in ASR. The authors have successfully applied semi-supervised graph based learning (GBL-SSL) to speech processing. 
The advantages of semi-supervised learning techniques over the traditional methods are plenty. Firstly, labelled data is hard to obtain and in some cases quite expensive too. For example, in speech processing, training data can be obtained easily, but annotating a large amount of data at phoneme level will be time-taking and error-prone too. 



%This study is focused on the latency introduced by the use of context frames as input to the acoustic models.
Nearly all recent methods that use DNN based acoustic models for ASR employ symmetric context windows as input \cite{Mohamed12, pdnn, dbn09, YaoEtAl2012SLTadaptation, DahlEtAl2011ICASSP} and are therefore affected by a certain latency.
In this study, we want to determine the relationship between latency of the DNN model and its performance.
In order to do this, we analyse how the performance of the recogniser proposed in \cite{pdnn} varies as the alignment of the input context window is shifted back or forward in time. 

%We observe that the context window can be shifted backwards without noticeable reduction in accuracy as long as the current feature vector is included in the context window.
%Beyond that, the performance starts to gradually but steeply degrade.
%This study suggests that without modifying the ASR method in \cite{pdnn}, we can reduce the latency of at least 50 ms without any degradation of the results.
%We can reduce it even more if some degradation can be tolerated by the application.
%This reduction in latency, although small in size, can potentially improve the usability of ASR in all applications, but is especially important in those latency critical applications described above.

%%% Local Variables: 
%%% enable-local-variables: t
%%% ispell-local-dictionary: "british"
%%% mode: latex
%%% TeX-master: "dnnlatency"
%%% eval: (flyspell-mode)
%%% eval: (flyspell-buffer)
%%% End: 
